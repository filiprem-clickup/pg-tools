#!/usr/bin/env python3

"""The pgboom utility manipulates Postgres metadata.

For details, see https://github.com/filiprem/pg-tools/tree/master/pgboom.
"""

import argparse
import doctest
import inspect
from pathlib import Path
import re
import sys
from time import localtime, strftime

import psycopg2
import psycopg2.errorcodes


PROGNAME = 'pgboom'

ACTIONS = ('explode', 'implode', 'diff', 'cat', 'test')

ARGS = None

# Rules for producing object definitions:
# * 'list' query must produce nspname and objname.
# * full schema names must be included.
# * 'create' query must include trailing semicolon.
OBJECTMETA = {
    'SCHEMA': {
        'list': """
        SELECT n.nspname, n.nspname AS objname, n.oid
        FROM pg_catalog.pg_namespace n
        WHERE n.nspname NOT IN ('pg_catalog', 'information_schema')
        AND n.nspname !~ '^pg_temp'
        AND n.nspname !~ '^pg_toast'
        """,
        'create': """SELECT pg_catalog.format('CREATE SCHEMA %I;', n.nspname)
        FROM pg_catalog.pg_namespace n
        WHERE n.oid = __OID__
        """,
    },
    'FUNCTION': {  # all functions except aggregate
        'list': """
        SELECT n.nspname, p.proname || '(' ||
        array_to_string(array(select (select t.typname from pg_type t where t.oid = a.oid) from unnest(p.proargtypes) a(oid)),',')
        || ')' AS objname, p.oid
        FROM pg_proc p, pg_namespace n
        WHERE n.oid = p.pronamespace
        AND p.prokind <> 'a'
        AND n.nspname NOT IN ('pg_catalog', 'information_schema')
        """,
        'create': """SELECT regexp_replace(pg_get_functiondef(__OID__),
                 '^CREATE OR REPLACE FUNCTION', 'CREATE FUNCTION') || ';'"""
    },
    'AGGREGATE': {  # aggregate functions only
        'list': """
        SELECT n.nspname, p.proname || '(' ||
        array_to_string(array(select (select t.typname from pg_type t where t.oid = a.oid) from unnest(p.proargtypes) a(oid)),',')
        || ')' AS objname, p.oid
        FROM pg_proc p, pg_namespace n
        WHERE n.oid = p.pronamespace
        AND p.prokind = 'a'
        AND n.nspname NOT IN ('pg_catalog', 'information_schema')
        """,
        # this is overriden later (see getaggdef method).
        # we just anticipate future built-in function name here.
        'create': """SELECT pg_get_aggregatedef(__OID__) || ';'"""
    },
    'SEQUENCE': {
        'list': """
        SELECT n.nspname, c.relname AS objname, c.oid
        FROM pg_class c, pg_namespace n
        WHERE n.oid = c.relnamespace AND c.relkind = 'S'
        AND n.nspname NOT IN ('pg_catalog', 'information_schema')
        """,
        'create': """
        SELECT format(E'CREATE SEQUENCE %I.%I AS %s\n'
                'INCREMENT %s MINVALUE %s MAXVALUE %s START %s CACHE %s%s;',
            n.nspname, c.relname, pg_catalog.format_type(sp.data_type, NULL),
            sp.increment, sp.minimum_value, sp.maximum_value, sp.start_value,
            sp.cache_size, case when sp.cycle_option then ' CYCLE' else '' end)
        FROM pg_catalog.pg_class c, pg_namespace n, pg_sequence_parameters(__OID__) sp
        WHERE c.oid = __OID__ AND n.oid = c.relnamespace
        """
    },
    'TABLE': {
        'list': """
        SELECT n.nspname, c.relname AS objname, c.oid
        FROM pg_class c, pg_namespace n
        WHERE n.oid = c.relnamespace AND c.relkind = 'r'
        AND n.nspname NOT IN ('pg_catalog', 'information_schema')
        """,
        # this is overriden later (see gettabledef method).
        # we just anticipate future built-in function name here.
        'create': """SELECT pg_get_tabledef(__OID__) || ';'"""
    },
    'CONSTRAINT': {
        'list': """
        SELECT n.nspname, con.conname AS objname, con.oid,
            crn.nspname AS conrelnspname, cr.relname AS conrelname
        FROM pg_constraint con, pg_namespace n, pg_class cr, pg_namespace crn
        WHERE n.oid = con.connamespace
        AND cr.oid = con.conrelid
        AND crn.oid = cr.relnamespace
        AND con.contype NOT IN ('u')
        AND n.nspname NOT IN ('pg_catalog', 'information_schema')
        """,
        'create': """SELECT format('ALTER TABLE %I.%I ADD CONSTRAINT %I %s;',
            '__CONRELNSPNAME__', '__CONRELNAME__', '__OBJECT__',
            pg_get_constraintdef(__OID__, true))"""
    },
    'INDEX': {
        'list': """
        SELECT n.nspname, c.relname AS objname, c.oid
        FROM pg_class c, pg_namespace n, pg_index i
        WHERE n.oid = c.relnamespace AND c.relkind = 'i' AND i.indexrelid = c.oid AND NOT i.indisprimary
        AND n.nspname NOT IN ('pg_catalog', 'information_schema')
        """,
        'create': """SELECT pg_get_indexdef(__OID__, 0, true) || ';'"""
    },
    'VIEW': {
        'list': """
        SELECT n.nspname, c.relname AS objname, c.oid
        FROM pg_class c, pg_namespace n
        WHERE n.oid = c.relnamespace AND c.relkind = 'v'
        AND n.nspname NOT IN ('pg_catalog', 'information_schema')
        """,
        'create': """
        SELECT E'CREATE VIEW __SCHEMA__.__OBJECT__ AS \\n' || pg_get_viewdef(__OID__, true)
        """
    },
}


def main():

    parser = argparse.ArgumentParser(
        prog=PROGNAME,
        description=__doc__,
        formatter_class=argparse.RawTextHelpFormatter,
    )

    actiondocs = []
    for a in ACTIONS:
        # grab the docstring from action's method
        adoc = globals()[a].__doc__
        # remove empty lines, usually present due to docstring conventions
        adoc = re.sub(r'\n\s*\n', '\n', adoc, count=1)
        # strip trailing newlines, if present
        adoc = re.sub(r'(\n\s*)+$', '', adoc)
        actiondocs.append(f'{a}: {adoc}\n')
    actionhelp = '\n'.join(actiondocs)

    # positional arguments: ACTION DSN DIR
    parser.add_argument('ACTION', choices=ACTIONS, metavar='ACTION', help=actionhelp)
    parser.add_argument('DSN', help="PostgreSQL data source in 'key=value' format")
    parser.add_argument('DIR', help="Destination/source directory")
    # options
    parser.add_argument('--Class', '-C', choices=OBJECTMETA.keys(),
                        help="Type of objects to extract/load")
    parser.add_argument('--Schema', '-S', help="Database schema name regex")
    parser.add_argument('--Object', '-O', help="Database object name regex")
    parser.add_argument('--File', '-F', help='Output file for the "cat" action')
    parser.add_argument('--debug', '--verbose', '-v', action='store_true',
                        help="Set verbose debugging on")

    global ARGS
    ARGS = parser.parse_args()

    debug(f'pgboom {ARGS.ACTION} starting')

    conn = None
    if ARGS.ACTION != 'cat':
        debug("testing database connection")
        try:
            conn = pgconn(ARGS.DSN)
        except BaseException:
            debug("connection failed. You are supposed to provide connection"
                  " parameters via environment variables [PGHOST, PGDATABASE etc],"
                  " as specified in https://www.postgresql.org/docs/current/libpq-envars.html",
                  'ERROR')
            raise

        with conn.cursor() as cur:
            cur.execute('SELECT current_database(), current_user, version()')
            (db, usr, ver) = cur.fetchone()
            debug(f'connection OK: user={usr}, db={db}, version={ver}')

    if ARGS.ACTION == 'explode':
        explode(conn)
    elif ARGS.ACTION == 'implode':
        implode(conn)
    elif ARGS.ACTION == 'diff':
        diff(conn)
    elif ARGS.ACTION == 'cat':
        cat()
    elif ARGS.ACTION == 'test':
        test()
    else:
        # this should never be reached, but will not hurt.
        help()

    debug(f'pgboom {ARGS.ACTION} finished')


def explode(conn: psycopg2.extensions.connection) -> dict:
    """Saves object definitions (SQL CREATE statements) from Postgres to directory.

    Each object goes into separate file, DIR/<CLASS>/<schema>/<object>.sql.
    """

    try:
        Path(ARGS.DIR).mkdir(parents=True, exist_ok=True)
    except OSError:
        debug(f'could not access output directory {ARGS.DIR}', 'ERROR')
        raise

    stats = {}

    for objclass in OBJECTMETA:
        if ARGS.Class is not None and objclass != ARGS.Class:
            continue
        count = explode_class(conn, objclass)
        stats[objclass] = count

    debug(f'finished, stats: {stats}', 'INFO')
    return stats


def implode(conn):
    """Loads object definitions from directory to Postgres.

    Does not overwrite any pre-existing objects.
    """

    root = Path(ARGS.DIR)
    if not (root.exists() and root.is_dir()):
        debug(f'directory {ARGS.DIR} does not exist', 'ERROR')
        sys.exit(1)

    counter = {}

    for objclass in OBJECTMETA:
        if ARGS.Class is not None and objclass != ARGS.Class:
            continue
        count = implode_class(conn, objclass)
        counter[objclass] = count

    debug(f'finished, stats: {counter}', 'INFO')


def diff(conn):
    """Compares object definitions between database and directory.

    The directory is suposed to be pre-populated with `pgboom explode`.
    """

    raise NotImplementedError


def cat():
    """Does the same as implode, but concatenates into --File, not database.
    """

    if ARGS.File is None:
        raise ValueError('The "cat" action requires --File option.')
    try:
        # overwrite the file
        open(ARGS.File, mode='w').close()
    except OSError as err:
        debug(err, 'ERROR')
        sys.exit(1)
    # implemented as part of implode.
    return implode(None)


def test():
    """Performs a built-in test (for developers).
    """

    debug("entering test code - please ignore ERROR messages "
          "unless you see specific tests failing", 'INFO')
    return doctest.testmod(verbose=ARGS.debug)


def make_where():
    """Construct a WHERE clause for object list filtering.

    Returns:
        A tuple with two items:
            1. SQL WHERE string with %s-style placeholders
            2. list of parameters for the placeholders.

    >>> ARGS.Schema = '^myschema$'
    >>> ARGS.Object = '^myobjectprefix'
    >>> print(make_where())
    (' WHERE nspname ~ %s AND objname ~ %s', ['^myschema$', '^myobjectprefix'])
    """

    conditions = []
    params = []
    clause = ''
    if ARGS.Schema is not None:
        conditions.append('nspname ~ %s')
        params.append(ARGS.Schema)
    if ARGS.Object is not None:
        conditions.append('objname ~ %s')
        params.append(ARGS.Object)
    if conditions:
        clause = ' WHERE ' + ' AND '.join(conditions)
    return (clause, params)


def explode_class(conn, objclass: str) -> int:
    """Extract objects of given class from database to flat files.

    Return counter.
    """

    debug(f'processing {objclass} definitions', 'INFO')
    count = 0
    with conn.cursor() as cur:
        sqlselect = OBJECTMETA[objclass]['list']
        sqlfilter, sqlparams = make_where()
        sqlquery = 'SELECT * FROM ({}) _lst{} ORDER BY nspname, objname'.format(
            sqlselect, sqlfilter)
        cur.execute(sqlquery, sqlparams)
        for r in cur:
            if objclass == 'CONSTRAINT':
                (nspname, objname, oid, conrelnspname, conrelname) = r
                objdef = getobjdef(
                    conn,
                    objclass,
                    oid,
                    nspname,
                    objname,
                    conrelnspname,
                    conrelname)
            else:
                (nspname, objname, oid) = r
                objdef = getobjdef(conn, objclass, oid, nspname, objname)
            fdir = f'{ARGS.DIR}/{objclass}/{nspname}'
            fname = f'{objname}.sql'
            debug(f'saving {fdir}/{fname}')
            Path(fdir).mkdir(parents=True, exist_ok=True)
            with open(f'{fdir}/{fname}', 'w') as of:
                of.write(objdef)
            count += 1

    return count or None


def implode_class(conn, objclass: str) -> int:
    """Load objects from directory.

    Objects go to database (action=implode) or file (action=cat).

    Returns:
        Counter of processed files.
    """

    # DIR/TABLE
    given_dir = Path(ARGS.DIR, objclass)
    if not (given_dir.exists() and given_dir.is_dir()):
        debug(f'directory {given_dir} does not exist', 'INFO')
        return None

    count = 0

    debug(f'processing {objclass} definitions', 'INFO')

    # DIR/TABLE/public
    for schema_dir in given_dir.iterdir():
        if not schema_dir.is_dir():
            continue
        schema = schema_dir.name
        if ARGS.Schema and not re.search(ARGS.Schema, schema):
            debug(f'ignoring {schema_dir} due to --Schema filter')
            continue
        # DIR/TABLE/public/users.sql
        for objectfile in schema_dir.glob('*.sql'):
            debug(f'processing {objectfile}')
            name = objectfile.stem
            if ARGS.Object and not re.search(ARGS.Object, name):
                debug(f'ignoring {objectfile} due to --Object filter')
                continue

            elif ARGS.ACTION == 'cat':
                with open(ARGS.File, mode='a') as ofd:
                    ofd.write(objectfile.read_text())
                    count += 1
            elif db_execute_ddl_file(conn, objectfile):
                count += 1

    return count or None


def is_expected_ddl(text: str) -> bool:
    """Try to verify if given text looks like SQL DDL.

    >>> is_expected_ddl('ALTER TABLE x ADD CONSTRAINT x_pkey PRIMARY KEY (id);')
    True
    >>> is_expected_ddl('DROP DATABASE mydb;')
    False
    """

    try:
        text = text.decode()
    except (UnicodeDecodeError, AttributeError):
        pass

    return bool(re.match('(CREATE|ALTER TABLE) ', text))


def db_execute_ddl_file(conn, filename) -> bool:
    """Executes DDL from given file in database.

    File content is only minimally validated: check if first line begins with
    relevant SQL keyword, ie. CREATE or ALTER.

    Args:
        conn: postgres db connection
        filename: path of file to be executed

    Returns:
        True on success, False on failure.

    >>> import tempfile
    >>> pg = pgconn(ARGS.DSN)
    >>> tmp1 = tempfile.NamedTemporaryFile(suffix='.sql', buffering=0)
    >>> tmp1.write('SELECT 1;'.encode())
    9
    >>> # Expecting failure:
    >>> db_execute_ddl_file(pg, tmp1.name)
    0
    >>> tmp2 = tempfile.NamedTemporaryFile(suffix='.sql', buffering=0)
    >>> tmp2.write('CREATE TEMP TABLE _testtable (id integer);'.encode())
    42
    >>> # Expecting success:
    >>> db_execute_ddl_file(pg, tmp2.name)
    1
    """

    file = Path(filename)

    text = file.read_text()
    if not is_expected_ddl(text):
        debug(f'unexpected content in "{file}"', 'ERROR')
        debug(f'unexpected content: {text}')
        return False
    with conn.cursor() as cur:
        # Explicit BEGIN is needed here, contrary to psycopg2 documentation.
        cur.execute('BEGIN')
        try:
            cur.execute(text)
            status = cur.statusmessage
            if not is_expected_ddl(status):
                raise UserWarning('Aborting transaction '
                                  f'due to "{status}" while executing "{file}"')
        except psycopg2.errors.ProgrammingError as e:
            errcode = psycopg2.errorcodes.lookup(e.pgcode)
            msg = e.diag.message_primary
            # detail = e.diag.message_detail
            # hint = e.diag.message_hint
            debug(f'{errcode} when executing {file}: {msg}', 'ERROR')
            return False
        finally:
            cur.execute('END')
    return True


def debug(message, level='DEBUG', stream=sys.stdout) -> None:
    """Emits a debug message.

    Args:
        message: message text.
        level: textual severity level
        stream: destination IO stream

    Some doctest notes:
        1. Need to specify stream, because doctest hijacks stdout before running.
        2. Due to leading '2' the test will start failing in year 3000.
        Hopefully, nobody will care then.

    >>> debug('elen silva lumen omentielvo', 'INFO', stream=sys.stdout) # doctest: +ELLIPSIS
    2... INFO ... elen silva lumen omentielvo
    >>> ARGS.debug = False
    >>> debug('"Dina" in Eldarin means "be silent".', stream=sys.stdout)
    """

    levels = {'ERROR': 10, 'INFO': 5, 'DEBUG': 0}
    if levels.get(level) is None:
        raise ValueError('wrong value for debug level: {}'.format(level))

    emit_level = levels['INFO']
    if ARGS.debug:
        emit_level = levels['DEBUG']

    if levels.get(level) < emit_level:
        return

    caller = inspect.stack()[1][3]

    ts = strftime("%F %X", localtime())

    stream.write(f'{ts} {level} {caller} {message}\n')
    stream.flush()


def pgconn(dsn='', autocommit=False):
    """Get PostgreSQL connection.

    Gets a new connection and sets application_name in the session.

    >>> pg = pgconn(ARGS.DSN)
    >>> pgcur = pg.cursor()
    >>> pgcur.execute('SELECT 2+3')
    >>> pgcur.fetchone()
    (5,)
    """

    conn = psycopg2.connect(dsn)
    conn.autocommit = autocommit
    with conn.cursor() as cur:
        cur.execute("SET application_name = %s", (PROGNAME,))
        # revert to standard search path to avoid side effects with
        # pg_get_indexdef and friends.
        cur.execute("SET search_path to public")
        # avoid function validation on implode
        cur.execute("SET check_function_bodies TO off")
    return conn


# Following functions exist to grab object definitions. They probably should be
# packaged into separate module.

def getobjdef(conn, objclass: str, oid: int, nspname: str = None, objname: str = None,
              conrelnspname: str = None, conrelname: str = None) -> str:
    """Get SQL object definition (CREATE statement) from database.

    Either the OID or (nspname, objname) pair will be required.

    >>> pg = pgconn(ARGS.DSN)
    >>> pgcur = pg.cursor()
    >>> pgcur.execute("create sequence _getobjdef_test"
    ...               " minvalue 1 maxvalue 1000 increment 10 start 1 cycle")
    >>> pgcur.execute("select oid from pg_class where relname='_getobjdef_test'")
    >>> oid = pgcur.fetchone()[0]
    >>> objdef = getobjdef(pg, 'SEQUENCE', oid, 'public', '_getobjdef_test')
    >>> print(objdef)
    CREATE SEQUENCE public._getobjdef_test AS bigint
    INCREMENT 10 MINVALUE 1 MAXVALUE 1000 START 1 CACHE 1 CYCLE;
    """
    with conn.cursor() as cur:
        objgen = OBJECTMETA[objclass]['create']
        objgen = re.sub('__OID__', str(oid), objgen)
        objgen = re.sub('__SCHEMA__', nspname, objgen)
        objgen = re.sub('__OBJECT__', objname, objgen)
        if objclass == 'CONSTRAINT':
            objgen = re.sub('__CONRELNSPNAME__', conrelnspname, objgen)
            objgen = re.sub('__CONRELNAME__', conrelname, objgen)
            cur.execute(objgen)
            return cur.fetchone()[0]
        elif objclass == 'TABLE':
            return gettabledef(conn, oid)
        elif objclass == 'AGGREGATE':
            return getaggdef(conn, oid)
        else:
            cur.execute(objgen)
            return cur.fetchone()[0]


def getaggdef(conn, aggoid: int) -> str:
    """Generate CREATE AGGREGATE statement for given aggregate function.

    Credits for SQL go to Erwin Brandstetter
    [https://stackoverflow.com/a/48575430/540341].

    >>> pg = pgconn(ARGS.DSN)
    >>> pgcur = pg.cursor()
    >>> pgcur.execute('''CREATE AGGREGATE _testagg (float8) (
    ...    sfunc = float8_accum, stype = float8[],
    ...    finalfunc = float8_avg, initcond = '{0,0,0}'
    ... )''')
    >>> pgcur.execute("SELECT '_testagg(float8)'::regprocedure::oid")
    >>> tempaggoid = pgcur.fetchone()[0]
    >>> print(getaggdef(pg, tempaggoid))
    CREATE AGGREGATE _testagg(double precision) (
        SFUNC = float8_accum, STYPE = double precision[],
        INITCOND = {0,0,0}, FINALFUNC = float8_avg);
    >>> pgcur.execute('''DROP AGGREGATE _testagg (float8)''')
    """

    aggdefsql = """
SELECT format(E'CREATE AGGREGATE %%s (\n    SFUNC = %%s, STYPE = %%s%%s%%s%%s%%s);'
    , aggfnoid::regprocedure
    , aggtransfn
    , aggtranstype::regtype
    , ', SORTOP = '    || NULLIF(aggsortop, 0)::regoper
    , E',\n    INITCOND = '  || agginitval
    , ', FINALFUNC = ' || NULLIF(aggfinalfn, 0)::regproc
    , CASE WHEN aggfinalextra THEN ', FINALFUNC_EXTRA' END
) AS ddl_agg
FROM pg_aggregate
WHERE aggfnoid = %(oid)s
    """

    with conn.cursor() as cur:
        cur.execute(aggdefsql, {'oid': aggoid})
        defn = cur.fetchone()[0]
        return defn


def gettabledef(conn, taboid: int) -> str:
    """Generate CREATE TABLE statement for given table.

    >>> pg = pgconn(ARGS.DSN)
    >>> pgcur = pg.cursor()
    >>> pgcur.execute('''CREATE TEMP TABLE gettabledef_testing (
    ...    intcol integer,
    ...    textcol text DEFAULT 'footext')
    ...    ''')
    >>> pgcur.execute("SELECT 'gettabledef_testing'::regclass::oid")
    >>> temptableoid = pgcur.fetchone()[0]
    >>> print(gettabledef(pg, temptableoid)) # doctest: +ELLIPSIS
    CREATE TEMP TABLE pg_temp_....gettabledef_testing (
        intcol integer,
        textcol text DEFAULT 'footext'::text
    );
    """

    with conn.cursor() as cur:
        versql = "SELECT current_setting('server_version_num')::integer"
        cur.execute(versql)
        ver = cur.fetchone()[0]
        if ver >= 120000:
            tabdefsql = """
WITH attrdef AS (
    SELECT
        n.nspname,
        c.relname,
        pg_catalog.array_to_string(c.reloptions || array(select 'toast.' || x from pg_catalog.unnest(tc.reloptions) x), ', ') as relopts,
        c.relpersistence,
        a.attnum,
        a.attname,
        pg_catalog.format_type(a.atttypid, a.atttypmod) as atttype,
        (SELECT substring(pg_catalog.pg_get_expr(d.adbin, d.adrelid, true) for 128) FROM pg_catalog.pg_attrdef d
            WHERE d.adrelid = a.attrelid AND d.adnum = a.attnum AND a.atthasdef) as attdefault,
        a.attnotnull,
        (SELECT c.collname FROM pg_catalog.pg_collation c, pg_catalog.pg_type t
            WHERE c.oid = a.attcollation AND t.oid = a.atttypid AND a.attcollation <> t.typcollation) as attcollation,
        a.attidentity,
        a.attgenerated
    FROM pg_catalog.pg_attribute a
    JOIN pg_catalog.pg_class c ON a.attrelid = c.oid
    JOIN pg_catalog.pg_namespace n ON c.relnamespace = n.oid
    LEFT JOIN pg_catalog.pg_class tc ON (c.reltoastrelid = tc.oid)
    WHERE a.attrelid = %(taboid)s
        AND a.attnum > 0
        AND NOT a.attisdropped
    ORDER BY a.attnum
),
coldef AS (
    SELECT
        attrdef.nspname,
        attrdef.relname,
        attrdef.relopts,
        attrdef.relpersistence,
        pg_catalog.format(
            '%%I %%s%%s%%s%%s%%s',
            attrdef.attname,
            attrdef.atttype,
            case when attrdef.attcollation is null then '' else pg_catalog.format(' COLLATE %%I', attrdef.attcollation) end,
            case when attrdef.attnotnull then ' NOT NULL' else '' end,
            case when attrdef.attdefault is null then ''
                else case when attrdef.attgenerated = 's' then pg_catalog.format(' GENERATED ALWAYS AS (%%s) STORED', attrdef.attdefault)
                    when attrdef.attgenerated <> '' then ' GENERATED AS NOT_IMPLEMENTED'
                    else pg_catalog.format(' DEFAULT %%s', attrdef.attdefault)
                end
            end,
            case when attrdef.attidentity<>'' then pg_catalog.format(' GENERATED %%s AS IDENTITY',
                    case attrdef.attidentity when 'd' then 'BY DEFAULT' when 'a' then 'ALWAYS' else 'NOT_IMPLEMENTED' end)
                else '' end
        ) as col_create_sql
    FROM attrdef
    ORDER BY attrdef.attnum
),
tabdef AS (
    SELECT
        coldef.nspname,
        coldef.relname,
        coldef.relopts,
        coldef.relpersistence,
        string_agg(coldef.col_create_sql, E',\n    ') as cols_create_sql
    FROM coldef
    GROUP BY
        coldef.nspname, coldef.relname, coldef.relopts, coldef.relpersistence
)
SELECT
    format(
        E'CREATE%%s TABLE %%I.%%I (\n    %%s\n)%%s;',
        case tabdef.relpersistence when 't' then ' TEMP' when 'u' then ' UNLOGGED' else '' end,
        tabdef.nspname,
        tabdef.relname,
        tabdef.cols_create_sql,
        case when tabdef.relopts <> '' then format(' WITH (%%s)', tabdef.relopts) else '' end
    ) as table_create_sql
FROM tabdef
            """
        else:
            tabdefsql = """
WITH attrdef AS (
    SELECT
        n.nspname,
        c.relname,
        pg_catalog.array_to_string(c.reloptions || array(select 'toast.' || x from pg_catalog.unnest(tc.reloptions) x), ', ') as relopts,
        c.relpersistence,
        a.attnum,
        a.attname,
        pg_catalog.format_type(a.atttypid, a.atttypmod) as atttype,
        (SELECT substring(pg_catalog.pg_get_expr(d.adbin, d.adrelid, true) for 128) FROM pg_catalog.pg_attrdef d
            WHERE d.adrelid = a.attrelid AND d.adnum = a.attnum AND a.atthasdef) as attdefault,
        a.attnotnull,
        (SELECT c.collname FROM pg_catalog.pg_collation c, pg_catalog.pg_type t
            WHERE c.oid = a.attcollation AND t.oid = a.atttypid AND a.attcollation <> t.typcollation) as attcollation,
        a.attidentity
    FROM pg_catalog.pg_attribute a
    JOIN pg_catalog.pg_class c ON a.attrelid = c.oid
    JOIN pg_catalog.pg_namespace n ON c.relnamespace = n.oid
    LEFT JOIN pg_catalog.pg_class tc ON (c.reltoastrelid = tc.oid)
    WHERE a.attrelid = %(taboid)s
        AND a.attnum > 0
        AND NOT a.attisdropped
    ORDER BY a.attnum
),
coldef AS (
    SELECT
        attrdef.nspname,
        attrdef.relname,
        attrdef.relopts,
        attrdef.relpersistence,
        pg_catalog.format(
            '%%I %%s%%s%%s%%s%%s',
            attrdef.attname,
            attrdef.atttype,
            case when attrdef.attcollation is null then '' else pg_catalog.format(' COLLATE %%I', attrdef.attcollation) end,
            case when attrdef.attnotnull then ' NOT NULL' else '' end,
            case when attrdef.attdefault is null then '' else pg_catalog.format(' DEFAULT %%s', attrdef.attdefault) end,
            case when attrdef.attidentity<>'' then pg_catalog.format(' GENERATED %%s AS IDENTITY',
                    case attrdef.attidentity when 'd' then 'BY DEFAULT' when 'a' then 'ALWAYS' else 'NOT_IMPLEMENTED' end)
                else '' end
        ) as col_create_sql
    FROM attrdef
    ORDER BY attrdef.attnum
),
tabdef AS (
    SELECT
        coldef.nspname,
        coldef.relname,
        coldef.relopts,
        coldef.relpersistence,
        string_agg(coldef.col_create_sql, E',\n    ') as cols_create_sql
    FROM coldef
    GROUP BY
        coldef.nspname, coldef.relname, coldef.relopts, coldef.relpersistence
)
SELECT
    format(
        E'CREATE%%s TABLE %%I.%%I (\n    %%s\n)%%s;',
        case tabdef.relpersistence when 't' then ' TEMP' when 'u' then ' UNLOGGED' else '' end,
        tabdef.nspname,
        tabdef.relname,
        tabdef.cols_create_sql,
        case when tabdef.relopts <> '' then format(' WITH (%%s)', tabdef.relopts) else '' end
    ) as table_create_sql
FROM tabdef
            """
        cur.execute(tabdefsql, {'taboid': taboid})
        defn = cur.fetchone()[0]
        return defn


if __name__ == "__main__":
    main()
