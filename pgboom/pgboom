#!/usr/bin/env python3

"""The pgboom utility manipulates Postgres metadata.

For details, see https://github.com/filiprem/pg-tools/tree/master/pgboom.
"""

import argparse
import doctest
import inspect
import pathlib
import re
import subprocess
import sys
import tempfile
import time

import psycopg2
import psycopg2.errorcodes
import psycopg2.extensions


PROGNAME = 'pgboom'

ACTIONS = ('explode', 'implode', 'diff', 'cat', 'test')

ARGS = None

# Rules for producing object definitions:
# * 'list' query must produce nspname and objname.
# * full schema names must be included.
# * 'create' query must include trailing semicolon.
OBJECTMETA = {
    'SCHEMA': {
        'list': """SELECT n.nspname, n.nspname AS objname, n.oid
        FROM pg_catalog.pg_namespace n
        WHERE n.nspname NOT IN ('pg_catalog', 'information_schema')
        AND n.nspname !~ '^pg_temp'
        AND n.nspname !~ '^pg_toast' """,
        'create': """SELECT pg_catalog.format('CREATE SCHEMA %I;', n.nspname)
        FROM pg_catalog.pg_namespace n WHERE n.oid = __OID__ """,
    },
    'EXTENSION': {
        'list': """SELECT n.nspname, x.extname AS objname, x.oid
        FROM pg_extension x
        JOIN pg_catalog.pg_namespace n ON n.oid = x.extnamespace
        WHERE x.extname <> 'plpgsql' """,
        'create': """SELECT pg_catalog.format(
            'CREATE EXTENSION %I SCHEMA %I CASCADE;', x.extname, n.nspname)
        FROM pg_catalog.pg_extension x
        JOIN pg_catalog.pg_namespace n ON n.oid = x.extnamespace
        WHERE x.oid = __OID__""",
    },
    'FUNCTION': {  # all functions except aggregate
        'list': """SELECT n.nspname, p.proname || '(' ||
        array_to_string(array(select (select t.typname from pg_type t where t.oid = a.oid) from unnest(p.proargtypes) a(oid)),',')
        || ')' AS objname, p.oid
        FROM pg_proc p, pg_namespace n
        WHERE n.oid = p.pronamespace
        AND p.prokind <> 'a'
        AND n.nspname NOT IN ('pg_catalog', 'information_schema')
        AND p.oid NOT IN (SELECT objid FROM pg_depend
                 WHERE refclassid = 'pg_extension'::regclass AND deptype = 'e') """,
        'create': """SELECT regexp_replace(pg_get_functiondef(__OID__),
                 '^CREATE OR REPLACE FUNCTION', 'CREATE FUNCTION') || ';'""",
    },
    'AGGREGATE': {  # aggregate functions only
        'list': """SELECT n.nspname, p.proname || '(' ||
        array_to_string(array(select (select t.typname from pg_type t where t.oid = a.oid) from unnest(p.proargtypes) a(oid)),',')
        || ')' AS objname, p.oid
        FROM pg_proc p, pg_namespace n
        WHERE n.oid = p.pronamespace
        AND p.prokind = 'a'
        AND n.nspname NOT IN ('pg_catalog', 'information_schema')
        AND p.oid NOT IN (SELECT objid FROM pg_depend
                 WHERE refclassid = 'pg_extension'::regclass AND deptype = 'e') """,
        # this is overriden later (see getaggdef method).
        # we just anticipate future built-in function name here.
        'create': """SELECT pg_get_aggregatedef(__OID__) || ';'""",
    },
    'SEQUENCE': {
        'list': """SELECT n.nspname, c.relname AS objname, c.oid
        FROM pg_class c, pg_namespace n
        WHERE n.oid = c.relnamespace AND c.relkind = 'S'
        AND n.nspname NOT IN ('pg_catalog', 'information_schema')
        AND c.oid NOT IN (SELECT objid FROM pg_depend
                 WHERE refclassid = 'pg_extension'::regclass AND deptype = 'e') """,
        'create': """SELECT format(E'CREATE SEQUENCE %I.%I AS %s\n'
                'INCREMENT %s MINVALUE %s MAXVALUE %s START %s CACHE %s%s;',
            n.nspname, c.relname, pg_catalog.format_type(sp.data_type, NULL),
            sp.increment, sp.minimum_value, sp.maximum_value, sp.start_value,
            sp.cache_size, case when sp.cycle_option then ' CYCLE' else '' end)
        FROM pg_catalog.pg_class c, pg_namespace n, pg_sequence_parameters(c.oid) sp
        WHERE c.oid = __OID__ AND n.oid = c.relnamespace """,
    },
    'TABLE': {
        'list': """SELECT n.nspname, c.relname AS objname, c.oid
        FROM pg_class c, pg_namespace n
        WHERE n.oid = c.relnamespace AND c.relkind = 'r'
        AND n.nspname NOT IN ('pg_catalog', 'information_schema')
        AND c.oid NOT IN (SELECT objid FROM pg_depend
                 WHERE refclassid = 'pg_extension'::regclass AND deptype = 'e') """,
        # this is overriden later (see gettabledef method).
        # we just anticipate future built-in function name here.
        'create': """SELECT pg_get_tabledef(__OID__) || ';'""",
    },
    'SEQUENCE_OWNED_BY': {
        # exploiting (documented) fact, that sequence can be owned by
        # same-schema table only.
        'list': """SELECT n.nspname, c.relname as objname, c.oid
        FROM pg_class c2 JOIN pg_depend d2 ON c2.oid=d2.refobjid JOIN pg_attribute a2
          ON (a2.attrelid=c2.oid AND a2.attnum=d2.refobjsubid)
        JOIN pg_class c ON c.oid = d2.objid JOIN pg_namespace n on n.oid = c.relnamespace
        WHERE d2.classid='pg_class'::regclass AND d2.refclassid='pg_class'::regclass
        AND d2.objid = c.oid AND d2.deptype='a' AND c.relkind = 'S' """,
        'create': """SELECT format('ALTER SEQUENCE %I.%I OWNED BY %I.%I.%I;',
            n.nspname, c.relname, n.nspname, c2.relname, a2.attname)
        FROM pg_class c2 JOIN pg_depend d2 ON c2.oid=d2.refobjid JOIN pg_attribute a2
          ON (a2.attrelid=c2.oid AND a2.attnum=d2.refobjsubid)
        JOIN pg_class c ON c.oid = d2.objid JOIN pg_namespace n on n.oid = c.relnamespace
        WHERE d2.classid='pg_class'::regclass AND d2.refclassid='pg_class'::regclass
        AND d2.objid = c.oid AND d2.deptype='a' AND c.relkind = 'S' AND c.oid = __OID__ """,
    },
    # This is separated from other CONSTRAINTs to work around PK->FK dependency problem.
    'PRIMARY_KEY': {
        'list': """SELECT n.nspname, con.conname AS objname, con.oid
        FROM pg_constraint con, pg_namespace n
        WHERE con.contype = 'p'
        AND n.oid = con.connamespace
        AND n.nspname NOT IN ('pg_catalog', 'information_schema') """,
        'create': """SELECT format('ALTER TABLE %I.%I ADD CONSTRAINT %I %s;',
            '__CONRELNSPNAME__', '__CONRELNAME__', '__OBJECT__',
            pg_get_constraintdef(__OID__, true))""",
    },
    'CONSTRAINT': {
        'list': """SELECT n.nspname, con.conname AS objname, con.oid,
            crn.nspname AS conrelnspname, cr.relname AS conrelname
        FROM pg_constraint con, pg_namespace n, pg_class cr, pg_namespace crn
        WHERE con.contype <> 'p'
        AND n.oid = con.connamespace
        AND cr.oid = con.conrelid
        AND crn.oid = cr.relnamespace
        AND n.nspname NOT IN ('pg_catalog', 'information_schema') """,
        'create': """SELECT format('ALTER TABLE %I.%I ADD CONSTRAINT %I %s;',
            '__CONRELNSPNAME__', '__CONRELNAME__', '__OBJECT__',
            pg_get_constraintdef(__OID__, true))""",
    },
    'INDEX': {
        'list': """SELECT n.nspname, c.relname AS objname, c.oid
        FROM pg_class c, pg_namespace n, pg_index i
        WHERE n.oid = c.relnamespace AND c.relkind = 'i'
        AND i.indexrelid = c.oid AND NOT i.indisprimary
        AND n.nspname NOT IN ('pg_catalog', 'information_schema')
        AND NOT EXISTS (SELECT 1 FROM pg_constraint con
            WHERE con.conrelid = i.indrelid AND con.conindid = i.indexrelid) """,
        'create': """SELECT pg_get_indexdef(__OID__, 0, true) || ';'""",
    },
    'VIEW': {
        'list': """SELECT n.nspname, c.relname AS objname, c.oid
        FROM pg_class c, pg_namespace n
        WHERE n.oid = c.relnamespace AND c.relkind = 'v'
        AND n.nspname NOT IN ('pg_catalog', 'information_schema')
        AND c.oid NOT IN (SELECT objid FROM pg_depend
                 WHERE refclassid = 'pg_extension'::regclass AND deptype = 'e') """,
        'create': """SELECT E'CREATE VIEW __SCHEMA__.__OBJECT__ AS \\n'
            || pg_get_viewdef(__OID__, true)""",
    },
}


def main():

    parser = argparse.ArgumentParser(
        prog=PROGNAME,
        description=__doc__,
        formatter_class=argparse.RawTextHelpFormatter,
    )

    actiondocs = []
    for a in ACTIONS:
        # grab the docstring from action's method
        adoc = globals()[a].__doc__
        # remove empty lines, usually present due to docstring conventions
        adoc = re.sub(r'\n\s*\n', '\n', adoc, count=1)
        # strip trailing newlines, if present
        adoc = re.sub(r'(\n\s*)+$', '', adoc)
        actiondocs.append(f'{a}: {adoc}\n')
    actionhelp = '\n'.join(actiondocs)

    # positional arguments: ACTION DSN DIR
    parser.add_argument('ACTION', choices=ACTIONS, metavar='ACTION', help=actionhelp)
    parser.add_argument('DSN', help="PostgreSQL data source in 'key=value' format")
    parser.add_argument('DIR', help="Destination/source directory")
    # options
    parser.add_argument('--Class', '-C', choices=OBJECTMETA.keys(),
                        help="Type of objects to extract/load")
    parser.add_argument('--Schema', '-S', help="Database schema name regex")
    parser.add_argument('--Object', '-O', help="Database object name regex")
    parser.add_argument('--File', '-F', help='Output file for `cat` and `diff` actions')
    parser.add_argument('--debug', '--verbose', '-v', action='store_true',
                        help="Set verbose debugging on")

    global ARGS
    ARGS = parser.parse_args()

    debug(f'pgboom {ARGS.ACTION} starting')

    conn = None
    if ARGS.ACTION != 'cat':
        debug("testing database connection")
        try:
            conn = pgconn(ARGS.DSN)
        except BaseException:
            debug("connection failed. You are supposed to provide connection"
                  " parameters via environment variables [PGHOST, PGDATABASE etc],"
                  " as specified in https://www.postgresql.org/docs/current/libpq-envars.html",
                  'ERROR')
            raise

        with conn.cursor() as cur:
            cur.execute('SELECT current_database(), current_user, version()')
            (db, usr, ver) = cur.fetchone()
            debug(f'connection OK: user={usr}, db={db}, version={ver}')

    if ARGS.ACTION == 'explode':
        res = explode(conn, ARGS.DIR)
    elif ARGS.ACTION == 'implode':
        res = implode(conn, ARGS.DIR)
    elif ARGS.ACTION == 'diff':
        res = diff(conn, ARGS.DIR)
    elif ARGS.ACTION == 'cat':
        res = cat(ARGS.DIR)
    elif ARGS.ACTION == 'test':
        res = test()
    else:
        raise NotImplementedError

    debug(f'pgboom {ARGS.ACTION} finished, result: {res}')


def verify_metadir(adir: str) -> bool:
    """Verifies metadata directory existence and minimal contents.
    """

    p = pathlib.Path(adir)
    if not (p.exists() and p.is_dir()):
        debug(f'directory {adir} does not exist', 'ERROR')
        sys.exit(2)
    subdirs = [x.name for x in p.iterdir() if x.is_dir()]
    if set(subdirs).isdisjoint(OBJECTMETA.keys()):
        debug(f'directory {adir} does not look like metadata directory', 'ERROR')
        sys.exit(2)
    return True


def explode(conn: psycopg2.extensions.connection, metadir: str) -> dict:
    """Saves object definitions (SQL CREATE statements) from Postgres to directory.

    Each object goes into separate file, DIR/<CLASS>/<schema>/<object>.sql.
    """

    try:
        pathlib.Path(metadir).mkdir(parents=True, exist_ok=True)
    except OSError:
        debug(f'could not access output directory {metadir}', 'ERROR')
        raise

    stats = {}

    for objclass in OBJECTMETA:
        if ARGS.Class is not None and objclass != ARGS.Class:
            continue
        count = explode_objects(conn, metadir, objclass)
        stats[objclass] = count

    debug(f'finished, stats: {stats}', 'INFO')
    return stats


def implode(conn, metadir):
    """Loads object definitions from directory to Postgres.

    Does not overwrite any pre-existing objects.
    """

    verify_metadir(metadir)
    stats = {}

    for objclass in OBJECTMETA:
        if ARGS.Class is not None and objclass != ARGS.Class:
            continue
        count = implode_objects(conn, metadir, objclass)
        stats[objclass] = count

    debug(f'finished, stats: {stats}', 'INFO')
    return stats


def diff(conn, metadir):
    """Compares object definitions between database and directory, using system `diff` command.

    The directory should have same structure as produced by `pgboom explode`.
    Results are saved to --File, in context diff format.
    """

    verify_metadir(metadir)

    if ARGS.File is None:
        debug('The `diff` action requires --File option', 'ERROR')
        sys.exit(1)
    try:
        ofd = open(ARGS.File, mode='w')
    except OSError as err:
        debug(err, 'ERROR')
        sys.exit(1)

    with tempfile.TemporaryDirectory() as dbdir:
        debug(f'exploding database to {dbdir}', 'INFO')
        explode(conn, dbdir)
        debug(f'comparing {dbdir} to {metadir}', 'INFO')
        for objclass in OBJECTMETA:
            dbsubdir = pathlib.Path(dbdir, objclass)
            metasubdir = pathlib.Path(metadir, objclass)
            if dbsubdir.exists() or metasubdir.exists():
                debug(f'processing {objclass} definitions')
                subprocess.run(['diff', '-r', '-C3', '-N', dbsubdir,
                                metasubdir], stdout=ofd, check=False)

        # Return number of bytes written to --File.
        return ofd.tell()


def cat(metadir: str) -> dict:
    """Does the same as implode, but concatenates into --File, not database.
    """

    if ARGS.File is None:
        raise ValueError('The "cat" action requires --File option.')
    try:
        # overwrite the file
        ofd = open(ARGS.File, mode='w')
        ofd.write('SET check_function_bodies TO off;\n\n')
        ofd.close()
    except OSError as err:
        debug(err, 'ERROR')
        sys.exit(1)
    # implemented as part of implode.
    return implode(None, metadir)


def test():
    """Performs a built-in test (for developers).
    """

    debug("entering test code - please ignore ERROR messages "
          "unless you see specific tests failing", 'INFO')
    return doctest.testmod(verbose=ARGS.debug)


def make_where():
    """Construct a WHERE clause for object list filtering.

    Returns:
        A tuple with two items:
            1. SQL WHERE string with %s-style placeholders
            2. list of parameters for the placeholders.

    >>> save_schema = ARGS.Schema
    >>> save_object = ARGS.Object
    >>> ARGS.Schema = '^myschema$'
    >>> ARGS.Object = '^myobjectprefix'
    >>> print(make_where())
    (' WHERE nspname ~ %s AND objname ~ %s', ['^myschema$', '^myobjectprefix'])
    >>> ARGS.Schema = save_schema
    >>> ARGS.Object = save_object
    """

    conditions = []
    params = []
    clause = ''
    if ARGS.Schema is not None:
        conditions.append('nspname ~ %s')
        params.append(ARGS.Schema)
    if ARGS.Object is not None:
        conditions.append('objname ~ %s')
        params.append(ARGS.Object)
    if conditions:
        clause = ' WHERE ' + ' AND '.join(conditions)
    return (clause, params)


def explode_objects(conn, metadir: str, objclass: str) -> int:
    """Extract objects of given class from database to flat files.

    Return counter.
    """

    debug(f'processing {objclass} definitions')
    count = 0
    with conn.cursor() as cur:
        sqlselect = OBJECTMETA[objclass]['list']
        sqlfilter, sqlparams = make_where()
        sqlquery = 'SELECT * FROM ({}) _lst{} ORDER BY nspname, objname'.format(
            sqlselect, sqlfilter)
        cur.execute(sqlquery, sqlparams)
        for r in cur:
            if objclass == 'CONSTRAINT':
                (nspname, objname, oid, conrelnspname, conrelname) = r
                objdef = getobjdef(
                    conn,
                    objclass,
                    oid,
                    nspname,
                    objname,
                    conrelnspname,
                    conrelname)
            else:
                (nspname, objname, oid) = r
                objdef = getobjdef(conn, objclass, oid, nspname, objname)
            fdir = f'{metadir}/{objclass}/{nspname}'
            fname = f'{objname}.sql'
            debug(f'saving {fdir}/{fname}')
            pathlib.Path(fdir).mkdir(parents=True, exist_ok=True)
            with open(f'{fdir}/{fname}', 'w') as of:
                of.write(objdef)
            count += 1

    return count or None


def implode_objects(conn, metadir: str, objclass: str) -> int:
    """Load objects from directory.

    Objects go to database (action=implode) or file (action=cat).

    Returns:
        Counter of processed files.
    """

    # DIR/TABLE
    objdir = pathlib.Path(metadir, objclass)
    if not (objdir.exists() and objdir.is_dir()):
        debug(f'directory {objdir} does not exist')
        return None

    count = 0

    debug(f'processing {objclass} definitions')

    # DIR/TABLE/public
    for schema_dir in objdir.iterdir():
        if not schema_dir.is_dir():
            continue
        schema = schema_dir.name
        if ARGS.Schema and not re.search(ARGS.Schema, schema):
            debug(f'ignoring {schema_dir} due to --Schema filter')
            continue
        # DIR/TABLE/public/users.sql
        for objectfile in schema_dir.glob('*.sql'):
            debug(f'processing {objectfile}')
            name = objectfile.stem
            if ARGS.Object and not re.search(ARGS.Object, name):
                debug(f'ignoring {objectfile} due to --Object filter')
                continue

            elif ARGS.ACTION == 'cat':
                with open(ARGS.File, mode='a') as ofd:
                    ofd.write(objectfile.read_text())
                    count += 1
            elif db_execute_ddl_file(conn, objectfile):
                count += 1

    return count or None


def is_expected_ddl(text: str) -> bool:
    """Try to verify if given text looks like SQL DDL.

    >>> is_expected_ddl('ALTER TABLE x ADD CONSTRAINT x_pkey PRIMARY KEY (id);')
    True
    >>> is_expected_ddl('DROP DATABASE mydb;')
    False
    """

    try:
        text = text.decode()
    except (UnicodeDecodeError, AttributeError):
        pass

    return bool(re.match(r'(CREATE|ALTER TABLE|ALTER SEQUENCE)\b', text))


def db_execute_ddl_file(conn, filename) -> bool:
    """Executes DDL from given file in database.

    File content is only minimally validated: check if first line begins with
    relevant SQL keyword, ie. CREATE or ALTER.

    Args:
        conn: postgres db connection
        filename: path of file to be executed

    Returns:
        True on success, False on failure.

    >>> pg = pgconn(ARGS.DSN)
    >>> tmp1 = tempfile.NamedTemporaryFile(suffix='.sql', buffering=0)
    >>> tmp1.write('SELECT 1;'.encode())
    9
    >>> # Expecting failure:
    >>> db_execute_ddl_file(pg, tmp1.name)
    0
    >>> tmp2 = tempfile.NamedTemporaryFile(suffix='.sql', buffering=0)
    >>> tmp2.write('CREATE TEMP TABLE _testtable (id integer);'.encode())
    42
    >>> # Expecting success:
    >>> db_execute_ddl_file(pg, tmp2.name)
    1
    """

    file = pathlib.Path(filename)

    text = file.read_text()
    if not is_expected_ddl(text):
        debug(f'unexpected content in "{file}"', 'ERROR')
        debug(f'unexpected content: {text}')
        return False
    with conn.cursor() as cur:
        # Explicit BEGIN is needed here, contrary to psycopg2 documentation.
        cur.execute('BEGIN')
        try:
            cur.execute(text)
            status = cur.statusmessage
            if not is_expected_ddl(status):
                raise UserWarning('Aborting transaction '
                                  f'due to "{status}" while executing "{file}"')
        except psycopg2.errors.ProgrammingError as e:
            errcode = psycopg2.errorcodes.lookup(e.pgcode)
            msg = e.diag.message_primary
            # detail = e.diag.message_detail
            # hint = e.diag.message_hint
            debug(f'{errcode} when executing {file}: {msg}', 'ERROR')
            return False
        finally:
            cur.execute('END')
    return True


def debug(message, level='DEBUG', stream=sys.stdout) -> None:
    """Emits a debug message.

    Args:
        message: message text.
        level: textual severity level
        stream: destination IO stream

    Some doctest notes:
        1. Need to specify stream, because doctest hijacks stdout before running.
        2. Due to leading '2' the test will start failing in year 3000.
        Hopefully, nobody will care then.

    >>> debug('elen silva lumen omentielvo', 'INFO', stream=sys.stdout) # doctest: +ELLIPSIS
    2... INFO ... elen silva lumen omentielvo
    >>> save_debug = ARGS.debug
    >>> ARGS.debug = False
    >>> debug('"Dina" in Eldarin means "be silent".', stream=sys.stdout)
    >>> ARGS.debug = save_debug
    """

    levels = {'ERROR': 10, 'INFO': 5, 'DEBUG': 0}
    if levels.get(level) is None:
        raise ValueError('wrong value for debug level: {}'.format(level))

    emit_level = levels['INFO']
    if ARGS.debug:
        emit_level = levels['DEBUG']

    if levels.get(level) < emit_level:
        return

    caller = inspect.stack()[1][3]

    ts = time.strftime("%F %X", time.localtime())

    stream.write(f'{ts} {level} {caller} {message}\n')
    stream.flush()


def pgconn(dsn='', autocommit=False):
    """Connects to PostgreSQL.

    Gets a new connection and sets application_name in the session.

    >>> pg = pgconn(ARGS.DSN)
    >>> pgcur = pg.cursor()
    >>> pgcur.execute('SELECT 2+3')
    >>> pgcur.fetchone()
    (5,)
    """

    conn = psycopg2.connect(dsn)
    conn.autocommit = autocommit
    with conn.cursor() as cur:
        cur.execute("SET application_name = %s", (PROGNAME,))
        # revert to standard search path to avoid side effects with
        # pg_get_indexdef and friends.
        cur.execute("SET search_path to public")
        # avoid function validation on implode
        cur.execute("SET check_function_bodies TO off")
        conn.commit()
    return conn


# Following functions exist to grab object definitions. They probably should be
# packaged into separate module.

def getobjdef(conn, objclass: str, oid: int, nspname: str = None, objname: str = None,
              conrelnspname: str = None, conrelname: str = None) -> str:
    """Gets SQL object definition (CREATE statement) from database.

    Either the OID or (nspname, objname) pair will be required.

    Args:
        conn: Pg connection
        objclass: object type (TABLE, FUNCTION, CONSTRAINT etc)
        oid: object OID
        nspname: schema name in database
        objname: unique object name within schema (for functions, this means
            full signature including argument types)
        conrelnspname: referring object schema
        conrelname: referring object name

    Returns:
        Object definition in form of CREATE statement.

    >>> pg = pgconn(ARGS.DSN)
    >>> pgcur = pg.cursor()
    >>> pgcur.execute("create sequence _getobjdef_test"
    ...               " minvalue 1 maxvalue 1000 increment 10 start 1 cycle")
    >>> pgcur.execute("select oid from pg_class where relname='_getobjdef_test'")
    >>> oid = pgcur.fetchone()[0]
    >>> objdef = getobjdef(pg, 'SEQUENCE', oid, 'public', '_getobjdef_test')
    >>> print(objdef)
    CREATE SEQUENCE public._getobjdef_test AS bigint
    INCREMENT 10 MINVALUE 1 MAXVALUE 1000 START 1 CACHE 1 CYCLE;
    <BLANKLINE>
    """
    with conn.cursor() as cur:
        objgen = OBJECTMETA[objclass]['create']
        objgen = re.sub('__OID__', str(oid), objgen)
        objgen = re.sub('__SCHEMA__', nspname, objgen)
        objgen = re.sub('__OBJECT__', objname, objgen)
        if objclass == 'CONSTRAINT':
            objgen = re.sub('__CONRELNSPNAME__', conrelnspname, objgen)
            objgen = re.sub('__CONRELNAME__', conrelname, objgen)
            cur.execute(objgen)
            objdef = cur.fetchone()[0]
        elif objclass == 'TABLE':
            objdef = gettabledef(conn, oid)
        elif objclass == 'AGGREGATE':
            objdef = getaggdef(conn, oid)
        else:
            cur.execute(objgen)
            objdef = cur.fetchone()[0]
    # make sure the definition has trailing newline
    if not re.search(r'\n\Z', objdef):
        objdef += '\n'
    return objdef


def getaggdef(conn, aggoid: int) -> str:
    """Generate CREATE AGGREGATE statement for given aggregate function.

    Credits for initial SQL go to Erwin Brandstetter
    [https://stackoverflow.com/a/48575430/540341].

    >>> pg = pgconn(ARGS.DSN)
    >>> pgcur = pg.cursor()
    >>> pgcur.execute('''CREATE AGGREGATE _testagg (float8) (
    ...    sfunc = float8_accum, stype = float8[],
    ...    finalfunc = float8_avg, initcond = '{0,0,0}'
    ... )''')
    >>> pgcur.execute("SELECT '_testagg(float8)'::regprocedure::oid")
    >>> tempaggoid = pgcur.fetchone()[0]
    >>> print(getaggdef(pg, tempaggoid))
    CREATE AGGREGATE _testagg(double precision) (
        SFUNC = float8_accum, STYPE = double precision[],
        INITCOND = {0,0,0}, FINALFUNC = float8_avg);
    >>> pgcur.execute('''DROP AGGREGATE _testagg (float8)''')
    """

    aggdefsql = """
SELECT format(E'CREATE AGGREGATE %%s (\n    SFUNC = %%s, STYPE = %%s%%s%%s%%s%%s);'
    , aggfnoid::regprocedure
    , aggtransfn
    , aggtranstype::regtype
    , ', SORTOP = '    || NULLIF(aggsortop, 0)::regoper
    , E',\n    INITCOND = '  || agginitval
    , ', FINALFUNC = ' || NULLIF(aggfinalfn, 0)::regproc
    , CASE WHEN aggfinalextra THEN ', FINALFUNC_EXTRA' END
) AS ddl_agg
FROM pg_aggregate
WHERE aggfnoid = %(oid)s
    """

    with conn.cursor() as cur:
        cur.execute(aggdefsql, {'oid': aggoid})
        defn = cur.fetchone()[0]
        return defn


def gettabledef(conn, taboid: int) -> str:
    """Generate CREATE TABLE statement for given table.

    >>> pg = pgconn(ARGS.DSN)
    >>> pgcur = pg.cursor()
    >>> pgcur.execute('''CREATE TEMP TABLE gettabledef_testing (
    ...    intcol integer,
    ...    textcol text DEFAULT 'footext')
    ...    ''')
    >>> pgcur.execute("SELECT 'gettabledef_testing'::regclass::oid")
    >>> temptableoid = pgcur.fetchone()[0]
    >>> print(gettabledef(pg, temptableoid)) # doctest: +ELLIPSIS
    CREATE TEMP TABLE pg_temp_....gettabledef_testing (
        intcol integer,
        textcol text DEFAULT 'footext'::text
    );
    """

    with conn.cursor() as cur:
        versql = "SELECT current_setting('server_version_num')::integer"
        cur.execute(versql)
        ver = cur.fetchone()[0]
        if ver >= 120000:
            tabdefsql = """
WITH attrdef AS (
    SELECT
        n.nspname,
        c.relname,
        pg_catalog.array_to_string(c.reloptions || array(select 'toast.' || x from pg_catalog.unnest(tc.reloptions) x), ', ') as relopts,
        c.relpersistence,
        a.attnum,
        a.attname,
        pg_catalog.format_type(a.atttypid, a.atttypmod) as atttype,
        (SELECT substring(pg_catalog.pg_get_expr(d.adbin, d.adrelid, true) for 128) FROM pg_catalog.pg_attrdef d
            WHERE d.adrelid = a.attrelid AND d.adnum = a.attnum AND a.atthasdef) as attdefault,
        a.attnotnull,
        (SELECT c.collname FROM pg_catalog.pg_collation c, pg_catalog.pg_type t
            WHERE c.oid = a.attcollation AND t.oid = a.atttypid AND a.attcollation <> t.typcollation) as attcollation,
        a.attidentity,
        a.attgenerated
    FROM pg_catalog.pg_attribute a
    JOIN pg_catalog.pg_class c ON a.attrelid = c.oid
    JOIN pg_catalog.pg_namespace n ON c.relnamespace = n.oid
    LEFT JOIN pg_catalog.pg_class tc ON (c.reltoastrelid = tc.oid)
    WHERE a.attrelid = %(taboid)s
        AND a.attnum > 0
        AND NOT a.attisdropped
    ORDER BY a.attnum
),
coldef AS (
    SELECT
        attrdef.nspname,
        attrdef.relname,
        attrdef.relopts,
        attrdef.relpersistence,
        pg_catalog.format(
            '%%I %%s%%s%%s%%s%%s',
            attrdef.attname,
            attrdef.atttype,
            case when attrdef.attcollation is null then '' else pg_catalog.format(' COLLATE %%I', attrdef.attcollation) end,
            case when attrdef.attnotnull then ' NOT NULL' else '' end,
            case when attrdef.attdefault is null then ''
                else case when attrdef.attgenerated = 's' then pg_catalog.format(' GENERATED ALWAYS AS (%%s) STORED', attrdef.attdefault)
                    when attrdef.attgenerated <> '' then ' GENERATED AS NOT_IMPLEMENTED'
                    else pg_catalog.format(' DEFAULT %%s', attrdef.attdefault)
                end
            end,
            case when attrdef.attidentity<>'' then pg_catalog.format(' GENERATED %%s AS IDENTITY',
                    case attrdef.attidentity when 'd' then 'BY DEFAULT' when 'a' then 'ALWAYS' else 'NOT_IMPLEMENTED' end)
                else '' end
        ) as col_create_sql
    FROM attrdef
    ORDER BY attrdef.attnum
),
tabdef AS (
    SELECT
        coldef.nspname,
        coldef.relname,
        coldef.relopts,
        coldef.relpersistence,
        string_agg(coldef.col_create_sql, E',\n    ') as cols_create_sql
    FROM coldef
    GROUP BY
        coldef.nspname, coldef.relname, coldef.relopts, coldef.relpersistence
)
SELECT
    format(
        E'CREATE%%s TABLE %%I.%%I (\n    %%s\n)%%s;',
        case tabdef.relpersistence when 't' then ' TEMP' when 'u' then ' UNLOGGED' else '' end,
        tabdef.nspname,
        tabdef.relname,
        tabdef.cols_create_sql,
        case when tabdef.relopts <> '' then format(' WITH (%%s)', tabdef.relopts) else '' end
    ) as table_create_sql
FROM tabdef
            """
        else:
            tabdefsql = """
WITH attrdef AS (
    SELECT
        n.nspname,
        c.relname,
        pg_catalog.array_to_string(c.reloptions || array(select 'toast.' || x from pg_catalog.unnest(tc.reloptions) x), ', ') as relopts,
        c.relpersistence,
        a.attnum,
        a.attname,
        pg_catalog.format_type(a.atttypid, a.atttypmod) as atttype,
        (SELECT substring(pg_catalog.pg_get_expr(d.adbin, d.adrelid, true) for 128) FROM pg_catalog.pg_attrdef d
            WHERE d.adrelid = a.attrelid AND d.adnum = a.attnum AND a.atthasdef) as attdefault,
        a.attnotnull,
        (SELECT c.collname FROM pg_catalog.pg_collation c, pg_catalog.pg_type t
            WHERE c.oid = a.attcollation AND t.oid = a.atttypid AND a.attcollation <> t.typcollation) as attcollation,
        a.attidentity
    FROM pg_catalog.pg_attribute a
    JOIN pg_catalog.pg_class c ON a.attrelid = c.oid
    JOIN pg_catalog.pg_namespace n ON c.relnamespace = n.oid
    LEFT JOIN pg_catalog.pg_class tc ON (c.reltoastrelid = tc.oid)
    WHERE a.attrelid = %(taboid)s
        AND a.attnum > 0
        AND NOT a.attisdropped
    ORDER BY a.attnum
),
coldef AS (
    SELECT
        attrdef.nspname,
        attrdef.relname,
        attrdef.relopts,
        attrdef.relpersistence,
        pg_catalog.format(
            '%%I %%s%%s%%s%%s%%s',
            attrdef.attname,
            attrdef.atttype,
            case when attrdef.attcollation is null then '' else pg_catalog.format(' COLLATE %%I', attrdef.attcollation) end,
            case when attrdef.attnotnull then ' NOT NULL' else '' end,
            case when attrdef.attdefault is null then '' else pg_catalog.format(' DEFAULT %%s', attrdef.attdefault) end,
            case when attrdef.attidentity<>'' then pg_catalog.format(' GENERATED %%s AS IDENTITY',
                    case attrdef.attidentity when 'd' then 'BY DEFAULT' when 'a' then 'ALWAYS' else 'NOT_IMPLEMENTED' end)
                else '' end
        ) as col_create_sql
    FROM attrdef
    ORDER BY attrdef.attnum
),
tabdef AS (
    SELECT
        coldef.nspname,
        coldef.relname,
        coldef.relopts,
        coldef.relpersistence,
        string_agg(coldef.col_create_sql, E',\n    ') as cols_create_sql
    FROM coldef
    GROUP BY
        coldef.nspname, coldef.relname, coldef.relopts, coldef.relpersistence
)
SELECT
    format(
        E'CREATE%%s TABLE %%I.%%I (\n    %%s\n)%%s;',
        case tabdef.relpersistence when 't' then ' TEMP' when 'u' then ' UNLOGGED' else '' end,
        tabdef.nspname,
        tabdef.relname,
        tabdef.cols_create_sql,
        case when tabdef.relopts <> '' then format(' WITH (%%s)', tabdef.relopts) else '' end
    ) as table_create_sql
FROM tabdef
            """
        cur.execute(tabdefsql, {'taboid': taboid})
        defn = cur.fetchone()[0]
        return defn


# test-only functions

def test_exp_imp():
    """Tests database reproduction with explode + implode.

    Uses `pg_dump -s` as comparison method.

    1. define temp names for dbs / files
    >>> db1 = f'tempdb{int(time.time())}'
    >>> db2 = f'{db1}_exp_imp'
    >>> dsn1 = f'{ARGS.DSN} dbname={db1}'
    >>> dsn2 = f'{ARGS.DSN} dbname={db2}'
    >>> exdir = f'{tempfile.gettempdir()}/{db1}'
    >>> file1 = f'{tempfile.gettempdir()}/{db1}.sql'
    >>> file2 = f'{tempfile.gettempdir()}/{db2}.sql'

    2. create dbs
    >>> conn = pgconn(ARGS.DSN, autocommit=True)
    >>> cur = conn.cursor()
    >>> cur.execute(f'CREATE DATABASE {db1}')
    >>> cur.execute(f'CREATE DATABASE {db2}')
    >>> conn.close()

    3. prepare db1
    >>> conn = pgconn(dsn1, autocommit=True)
    >>> cur = conn.cursor()
    >>> cur.execute('''
    ... CREATE SCHEMA s1
    ... CREATE TABLE t1 (id serial unique)
    ... CREATE VIEW v1 AS SELECT id FROM t1 WHERE id > 0
    ... ''')
    >>> conn.close()

    4. explode db1
    >>> conn = pgconn(dsn1)
    >>> explode(conn, exdir) # doctest: +NORMALIZE_WHITESPACE
    {'SCHEMA': 2, 'EXTENSION': None, 'FUNCTION': None, 'AGGREGATE': None,
     'SEQUENCE': 1, 'TABLE': 1, 'SEQUENCE_OWNED_BY': 1, 'PRIMARY_KEY': None,
     'CONSTRAINT': 1, 'INDEX': None, 'VIEW': 1}
    >>> conn.close()

    5. implode db2
    >>> conn = pgconn(dsn2)
    >>> implode(conn, exdir) # doctest: +NORMALIZE_WHITESPACE
    {'SCHEMA': 1, 'EXTENSION': None, 'FUNCTION': None, 'AGGREGATE': None,
     'SEQUENCE': 1, 'TABLE': 1, 'SEQUENCE_OWNED_BY': 1, 'PRIMARY_KEY': None,
     'CONSTRAINT': 1, 'INDEX': None, 'VIEW': 1}
    >>> conn.close()

    6. make dumps of both
    >>> proc1 = subprocess.run(['pg_dump', '-s', '-d', dsn1, '-f', file1,
    ...                         '--no-owner', '--no-acl'], check=True)
    >>> proc2 = subprocess.run(['pg_dump', '-s', '-d', dsn2, '-f', file2,
    ...                         '--no-owner', '--no-acl'], check=True)

    7. compare
    >>> import filecmp
    >>> filecmp.cmp(file1, file2, shallow=False)
    True

    8. cleanup
    >>> conn = pgconn(ARGS.DSN, autocommit=True)
    >>> cur = conn.cursor()
    >>> cur.execute(f'DROP DATABASE {db1}')
    >>> cur.execute(f'DROP DATABASE {db2}')
    >>> conn.close()
    """


if __name__ == "__main__":
    main()
