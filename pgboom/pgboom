#!/usr/bin/env python3

"""The pgboom utility manipulates Postgres metadata.

For details, use `--help` or consult project README, available at
https://github.com/filiprem/pg-tools/blob/master/pgboom/README.md
"""

import sys
import re
import argparse
import doctest
import inspect
from time import localtime, strftime
from pathlib import Path

import psycopg2
import psycopg2.errorcodes


PROGNAME = 'pgboom'

PROGDESC = """
Utility to extract object metadata (ie., database schema) from PostgreSQL
database to directory structure, and vice versa.

Each db object goes into separate file:
    `DIR/OBJECT-CLASS/SCHEMA-NAME/OBJECT-NAME.sql`.
"""

ARGS = None

# Rules for producing object definitions:
# * 'list' query must produce nspname and objname.
# * full schema names must be included.
# * 'create' query must include trailing semicolon.
METADATAOBJECTS = {
    'SCHEMA': {
        'list': """
        SELECT n.nspname, n.nspname AS objname, n.oid
        FROM pg_catalog.pg_namespace n
        WHERE n.nspname NOT IN ('pg_catalog', 'information_schema')
        AND n.nspname !~ '^pg_temp'
        AND n.nspname !~ '^pg_toast'
        """,
        'create': """SELECT pg_catalog.format('CREATE SCHEMA %I;', n.nspname)
        FROM pg_catalog.pg_namespace n
        WHERE n.oid = __OID__
        """,
    },
    'FUNCTION': {  # all functions except aggregate
        'list': """
        SELECT n.nspname, p.proname || '(' ||
        array_to_string(array(select (select t.typname from pg_type t where t.oid = a.oid) from unnest(p.proargtypes) a(oid)),',')
        || ')' AS objname, p.oid
        FROM pg_proc p, pg_namespace n
        WHERE n.oid = p.pronamespace
        AND p.prokind <> 'a'
        AND n.nspname NOT IN ('pg_catalog', 'information_schema')
        """,
        'create': """SELECT regexp_replace(pg_get_functiondef(__OID__),
                 '^CREATE OR REPLACE FUNCTION', 'CREATE FUNCTION') || ';'"""
    },
    'AGGREGATE': {  # aggregate functions only
        'list': """
        SELECT n.nspname, p.proname || '(' ||
        array_to_string(array(select (select t.typname from pg_type t where t.oid = a.oid) from unnest(p.proargtypes) a(oid)),',')
        || ')' AS objname, p.oid
        FROM pg_proc p, pg_namespace n
        WHERE n.oid = p.pronamespace
        AND p.prokind = 'a'
        AND n.nspname NOT IN ('pg_catalog', 'information_schema')
        """,
        # this is overriden later (see getaggdef method).
        # we just anticipate future built-in function name here.
        'create': """SELECT pg_get_aggregatedef(__OID__) || ';'"""
    },
    'SEQUENCE': {
        'list': """
        SELECT n.nspname, c.relname AS objname, c.oid
        FROM pg_class c, pg_namespace n
        WHERE n.oid = c.relnamespace AND c.relkind = 'S'
        AND n.nspname NOT IN ('pg_catalog', 'information_schema')
        """,
        'create': """
        SELECT format('CREATE SEQUENCE %I.%I AS %s '
                'INCREMENT %s MINVALUE %s MAXVALUE %s START %s CACHE %s%s;',
            n.nspname, c.relname, pg_catalog.format_type(sp.data_type, NULL),
            sp.increment, sp.minimum_value, sp.maximum_value, sp.start_value,
            sp.cache_size, case when sp.cycle_option then ' CYCLE' else '' end)
        FROM pg_catalog.pg_class c, pg_namespace n, pg_sequence_parameters(__OID__) sp
        WHERE c.oid = __OID__ AND n.oid = c.relnamespace
        """
    },
    'TABLE': {
        'list': """
        SELECT n.nspname, c.relname AS objname, c.oid
        FROM pg_class c, pg_namespace n
        WHERE n.oid = c.relnamespace AND c.relkind = 'r'
        AND n.nspname NOT IN ('pg_catalog', 'information_schema')
        """,
        # this is overriden later (see gettabledef method).
        # we just anticipate future built-in function name here.
        'create': """SELECT pg_get_tabledef(__OID__) || ';'"""
    },
    'CONSTRAINT': {
        'list': """
        SELECT n.nspname, con.conname AS objname, con.oid,
            crn.nspname AS conrelnspname, cr.relname AS conrelname
        FROM pg_constraint con, pg_namespace n, pg_class cr, pg_namespace crn
        WHERE n.oid = con.connamespace
        AND cr.oid = con.conrelid
        AND crn.oid = cr.relnamespace
        AND con.contype NOT IN ('u')
        AND n.nspname NOT IN ('pg_catalog', 'information_schema')
        """,
        'create': """SELECT format('ALTER TABLE %I.%I ADD CONSTRAINT %I %s;',
            '__CONRELNSPNAME__', '__CONRELNAME__', '__OBJECT__',
            pg_get_constraintdef(__OID__, true))"""
    },
    'INDEX': {
        'list': """
        SELECT n.nspname, c.relname AS objname, c.oid
        FROM pg_class c, pg_namespace n, pg_index i
        WHERE n.oid = c.relnamespace AND c.relkind = 'i' AND i.indexrelid = c.oid AND NOT i.indisprimary
        AND n.nspname NOT IN ('pg_catalog', 'information_schema')
        """,
        'create': """SELECT pg_get_indexdef(__OID__, 0, true) || ';'"""
    },
    'VIEW': {
        'list': """
        SELECT n.nspname, c.relname AS objname, c.oid
        FROM pg_class c, pg_namespace n
        WHERE n.oid = c.relnamespace AND c.relkind = 'v'
        AND n.nspname NOT IN ('pg_catalog', 'information_schema')
        """,
        'create': """
        SELECT E'CREATE VIEW __SCHEMA__.__OBJECT__ AS \\n' || pg_get_viewdef(__OID__, true)
        """
    },
}


def main():

    parser = argparse.ArgumentParser(description=PROGDESC)

    # positional arguments: ACTION DSN DIR
    actionhelp = """Action to perform.
        * "explode" will save object definitions from database to directory.
        * "implode" will do the opposite of "explode"
          (but will not overwrite any pre-existing objects).
        * "cat" will do the same as "implode", but write into --File, not database.
        * "test" will run a built-in test (for devs)."""
    parser.add_argument('ACTION', choices=('explode', 'implode', 'cat', 'test'), help=actionhelp)
    parser.add_argument('DSN', help="PostgreSQL data source in 'key=value' format")
    parser.add_argument('DIR', help="Destination/source directory")
    # options
    parser.add_argument('--Class', '-C', choices=METADATAOBJECTS.keys(),
                        help="Type of objects to extract/load")
    parser.add_argument('--Schema', '-S', help="Database schema name filter")
    parser.add_argument('--Object', '-O', help="Database object name filter")
    parser.add_argument('--File', '-F', help='Output file for the "cat" action')
    parser.add_argument('--test', action='store_true', help="Perform built-in test")
    parser.add_argument('--debug', '--verbose', '-v', action='store_true',
                        help="Set verbose debugging on")
    parser.add_argument('--dry-run', action='store_true',
                        help="Simulation mode (avoid any side effects)")

    global ARGS
    ARGS = parser.parse_args()

    debug(f'pgboom {ARGS.ACTION} starting')

    if ARGS.test or ARGS.ACTION == 'test':
        debug("entering test code - please ignore ERROR messages "
              "unless you see specific tests failing", 'INFO')
        doctest.testmod(verbose=ARGS.debug)
        sys.exit(0)

    if ARGS.dry_run:
        debug('running in simulation mode', 'INFO')

    conn = None
    if ARGS.ACTION == 'explode' or ARGS.ACTION == 'implode':
        debug("testing database connection")
        try:
            conn = pgconn(ARGS.DSN)
        except BaseException:
            debug("connection failed. You are supposed to provide connection"
                  " parameters via environment variables [PGHOST, PGDATABASE etc],"
                  " as specified in https://www.postgresql.org/docs/current/libpq-envars.html",
                  'ERROR')
            raise

        with conn.cursor() as cur:
            cur.execute('SELECT current_database(), current_user, version()')
            (db, usr, ver) = cur.fetchone()
            debug(f'connection OK: user={usr}, db={db}, version={ver}')

    if ARGS.ACTION == 'explode':
        explode(conn)
    elif ARGS.ACTION == 'implode':
        implode(conn)
    elif ARGS.ACTION == 'cat':
        if ARGS.File is None:
            raise ValueError('The "cat" action requires --File option.')
        try:
            # overwrite the file
            open(ARGS.File, mode='w').close()
        except OSError as err:
            debug(err, 'ERROR')
            sys.exit(1)
        cat()
    else:
        raise ValueError('Action not supported: ' + ARGS.ACTION)

    debug(f'pgboom {ARGS.ACTION} finished')


def cat():
    """Concatenate files from DIR same way as "implode" would.

    The output will go to a file specified by --File option.
    """

    # implemented as part of implode.
    return implode(None)


def explode(conn):
    """Get metadata from Postgres into directory.
    """
    try:
        if not ARGS.dry_run:
            Path(ARGS.DIR).mkdir(parents=True, exist_ok=True)
    except OSError:
        debug(f'could not access output directory {ARGS.DIR}', 'ERROR')
        raise

    counter = {}

    for objclass in METADATAOBJECTS:
        if ARGS.Class is not None and objclass != ARGS.Class:
            continue
        count = explode_class(conn, objclass)
        counter[objclass] = count

    debug(f'finished, stats: {counter}', 'INFO')


def implode(conn):
    """Get metadata from a directory into Postgres.
    """
    root = Path(ARGS.DIR)
    if not (root.exists() and root.is_dir()):
        debug(f'directory {ARGS.DIR} does not exist', 'ERROR')
        sys.exit(1)

    counter = {}

    for objclass in METADATAOBJECTS:
        if ARGS.Class is not None and objclass != ARGS.Class:
            continue
        count = implode_class(conn, objclass)
        counter[objclass] = count

    debug(f'finished, stats: {counter}', 'INFO')


def make_where():
    """Construct a WHERE clause for object list filtering.

    Return a tuple with two items:
        1. SQL WHERE string with %s-style placeholders
        2. list of parameters for the placeholders.

    >>> ARGS.Schema = '^myschema$'
    >>> ARGS.Object = '^myobjectprefix'
    >>> print(make_where())
    (' WHERE nspname ~ %s AND objname ~ %s', ['^myschema$', '^myobjectprefix'])
    """
    conditions = []
    params = []
    clause = ''
    if ARGS.Schema is not None:
        conditions.append('nspname ~ %s')
        params.append(ARGS.Schema)
    if ARGS.Object is not None:
        conditions.append('objname ~ %s')
        params.append(ARGS.Object)
    if conditions:
        clause = ' WHERE ' + ' AND '.join(conditions)
    return (clause, params)


def explode_class(conn, objclass):
    """Extract objects of given class from database to flat files.

    Return counter.
    """
    count = 0
    objsql = METADATAOBJECTS.get(objclass)
    if objsql is None:
        raise ValueError(f'Object class {objclass} not supported')

    debug(f'processing {objclass} definitions', 'INFO')

    with conn.cursor() as cur:
        sqlselect = objsql['list']
        sqlfilter, sqlparams = make_where()
        sqlquery = 'SELECT * FROM ({}) _lst{} ORDER BY nspname, objname'.format(
            sqlselect, sqlfilter)
        cur.execute(sqlquery, sqlparams)
        for r in cur:
            if objclass == 'CONSTRAINT':
                (nspname, objname, oid, conrelnspname, conrelname) = r
            else:
                (nspname, objname, oid) = r
            fdir = f'{ARGS.DIR}/{objclass}/{nspname}'
            fname = f'{objname}.sql'
            with conn.cursor() as cur2:
                objgen = objsql['create']
                objgen = re.sub('__OID__', str(oid), objgen)
                objgen = re.sub('__SCHEMA__', nspname, objgen)
                objgen = re.sub('__OBJECT__', objname, objgen)
                if objclass == 'CONSTRAINT':
                    objgen = re.sub('__CONRELNSPNAME__', conrelnspname, objgen)
                    objgen = re.sub('__CONRELNAME__', conrelname, objgen)
                    cur2.execute(objgen)
                    res = cur2.fetchone()[0]
                elif objclass == 'TABLE':
                    res = gettabledef(conn, oid)
                elif objclass == 'AGGREGATE':
                    res = getaggdef(conn, oid)
                else:
                    cur2.execute(objgen)
                    res = cur2.fetchone()[0]

                count += 1

                debug(f'going to save {fdir}/{fname}')
                if not ARGS.dry_run:
                    Path(fdir).mkdir(parents=True, exist_ok=True)
                    with open(f'{fdir}/{fname}', 'w') as of:
                        of.write(res)

    return count or None


def implode_class(conn, objclass):
    """Load objects from directory.
    
    Objects go to database (action=implode) or file (action=cat).

    Returns:
        Counter of processed files.
    Todo: overwrite?  dependencies?
    """
    # DIR/TABLE
    given_dir = Path(ARGS.DIR, objclass)
    if not (given_dir.exists() and given_dir.is_dir()):
        debug(f'directory {given_dir} does not exist', 'INFO')
        return None

    count = 0

    debug(f'processing {objclass} definitions', 'INFO')

    # DIR/TABLE/public
    for schema_dir in given_dir.iterdir():
        if not schema_dir.is_dir():
            continue
        schema = schema_dir.name
        if ARGS.Schema and not re.search(ARGS.Schema, schema):
            debug(f'ignoring {schema_dir} due to --Schema filter')
            continue
        # DIR/TABLE/public/users.sql
        for objectfile in schema_dir.glob('*.sql'):
            name = objectfile.stem
            if ARGS.Object and not re.search(ARGS.Object, name):
                debug(f'ignoring {objectfile} due to --Object filter')
                continue

            debug(f'processing {objectfile}', 'DEBUG')
            if ARGS.ACTION == 'cat':
                with open(ARGS.File, mode='a') as ofd:
                    ofd.write(objectfile.read_text())
                    count += 1
            elif db_execute_ddl_file(conn, objectfile):
                count += 1

    return count or None


def is_expected_ddl(text):
    """Try to verify if given text looks like SQL DDL.

    >>> is_expected_ddl('ALTER TABLE x ADD CONSTRAINT x_pkey PRIMARY KEY (id);')
    True
    >>> is_expected_ddl('DROP DATABASE mydb;')
    False
    """
    try:
        text = text.decode()
    except (UnicodeDecodeError, AttributeError):
        pass

    return bool(re.match('(CREATE|ALTER TABLE) ', text))


def db_execute_ddl_file(conn, filename) -> bool:
    """Executes DDL from given file in database.

    File content is only minimally validated: check if first line begins with
    relevant SQL keyword, ie. CREATE or ALTER.

    Args:
        conn: postgres db connection
        filename: path of file to be executed

    Returns:
        True on success, False on failure.

    >>> pg = pgconn('')
    >>> import tempfile
    >>> tmp1 = tempfile.NamedTemporaryFile(suffix='.sql', buffering=0)
    >>> tmp1.write('SELECT 1;'.encode())
    9
    >>> # Expecting failure:
    >>> db_execute_ddl_file(pg, tmp1.name)
    0
    >>> tmp2 = tempfile.NamedTemporaryFile(suffix='.sql', buffering=0)
    >>> tmp2.write('CREATE TEMP TABLE _testtable (id integer);'.encode())
    42
    >>> # Expecting success:
    >>> db_execute_ddl_file(pg, tmp2.name)
    1
    """

    file = Path(filename)

    text = file.read_text()
    if not is_expected_ddl(text):
        debug(f'unexpected content in "{file}"', 'ERROR')
        debug(f'unexpected content: {text}', 'DEBUG')
        return False
    with conn.cursor() as cur:
        # Explicit BEGIN is needed here, contrary to psycopg2 documentation.
        cur.execute('BEGIN')
        try:
            cur.execute(text)
            status = cur.statusmessage
            if not is_expected_ddl(status):
                raise UserWarning('Aborting transaction '
                                  f'due to "{status}" while executing "{file}"')
        except psycopg2.errors.ProgrammingError as e:
            errcode = psycopg2.errorcodes.lookup(e.pgcode)
            msg = e.diag.message_primary
            # detail = e.diag.message_detail
            # hint = e.diag.message_hint
            debug(f'{errcode} when executing {file}: {msg}', 'ERROR')
            return False
        finally:
            cur.execute('END')
    return True


def debug(message, level='DEBUG', stream=sys.stdout) -> None:
    """Emit a debug message.

    Args:
        message: message text.

    Some doctest notes:
        1. Need to specify stream, because doctest hijacks stdout before running.
        2. Due to leading '2' the test will start failing in year 3000.
        Hopefully, nobody will care then.

    >>> debug('elen silva lumen omentielvo', 'INFO', stream=sys.stdout) # doctest: +ELLIPSIS
    2... INFO ... elen silva lumen omentielvo
    >>> ARGS.debug = False
    >>> debug('"Dina" in Eldarin means "be silent".', stream=sys.stdout)
    """

    levels = {'ERROR': 10, 'INFO': 5, 'DEBUG': 0}
    if levels.get(level) is None:
        raise ValueError('wrong value for debug level: {}'.format(level))

    emit_level = levels['INFO']
    if ARGS.debug:
        emit_level = levels['DEBUG']

    if levels.get(level) < emit_level:
        return

    caller = inspect.stack()[1][3]

    ts = strftime("%F %X", localtime())

    stream.write(f'{ts} {level} {caller} {message}\n')
    stream.flush()


def pgconn(dsn='', autocommit=False):
    """Get PostgreSQL connection.

    Gets a new connection and sets application_name in the session.

    >>> pg = pgconn('')
    >>> pgcur = pg.cursor()
    >>> pgcur.execute('SELECT 2+3')
    >>> pgcur.fetchone()
    (5,)
    """
    conn = psycopg2.connect(dsn)
    conn.autocommit = autocommit
    with conn.cursor() as cur:
        cur.execute("SET application_name = %s", (PROGNAME,))
        # revert to standard search path to avoid side effects with
        # pg_get_indexdef and friends.
        cur.execute("SET search_path to public")
        # avoid function validation on implode
        cur.execute("SET check_function_bodies TO off")
    return conn


# Following functions exist to grab object definitions. They probably should be
# packaged into separate module.

def getaggdef(conn, aggoid: int) -> str:
    """Generate CREATE AGGREGATE statement for given aggregate function.

    Credits for SQL go to Erwin Brandstetter
    [https://stackoverflow.com/a/48575430/540341].

    >>> pg = pgconn('')
    >>> pgcur = pg.cursor()
    >>> pgcur.execute('''CREATE AGGREGATE _testagg (float8) (
    ...    sfunc = float8_accum, stype = float8[],
    ...    finalfunc = float8_avg, initcond = '{0,0,0}'
    ... )''')
    >>> pgcur.execute("SELECT '_testagg(float8)'::regprocedure::oid")
    >>> tempaggoid = pgcur.fetchone()[0]
    >>> print(getaggdef(pg, tempaggoid))
    CREATE AGGREGATE _testagg(double precision) (
        SFUNC = float8_accum, STYPE = double precision[],
        INITCOND = {0,0,0}, FINALFUNC = float8_avg);
    >>> pgcur.execute('''DROP AGGREGATE _testagg (float8)''')
    """

    aggdefsql = """
SELECT format(E'CREATE AGGREGATE %%s (\n    SFUNC = %%s, STYPE = %%s%%s%%s%%s%%s);'
    , aggfnoid::regprocedure
    , aggtransfn
    , aggtranstype::regtype
    , ', SORTOP = '    || NULLIF(aggsortop, 0)::regoper
    , E',\n    INITCOND = '  || agginitval
    , ', FINALFUNC = ' || NULLIF(aggfinalfn, 0)::regproc
    , CASE WHEN aggfinalextra THEN ', FINALFUNC_EXTRA' END
) AS ddl_agg
FROM pg_aggregate
WHERE aggfnoid = %(oid)s
    """

    with conn.cursor() as cur:
        cur.execute(aggdefsql, {'oid': aggoid})
        defn = cur.fetchone()[0]
        return defn


def gettabledef(conn, taboid: int) -> str:
    """Generate CREATE TABLE statement for given table.

    >>> pg = pgconn('')
    >>> pgcur = pg.cursor()
    >>> pgcur.execute('''CREATE TEMP TABLE gettabledef_testing (
    ...    intcol integer,
    ...    textcol text DEFAULT 'footext')
    ...    ''')
    >>> pgcur.execute("SELECT 'gettabledef_testing'::regclass::oid")
    >>> temptableoid = pgcur.fetchone()[0]
    >>> print(gettabledef(pg, temptableoid)) # doctest: +ELLIPSIS
    CREATE TEMP TABLE pg_temp_....gettabledef_testing (
        intcol integer,
        textcol text DEFAULT 'footext'::text
    );
    """
    with conn.cursor() as cur:
        versql = "SELECT current_setting('server_version_num')::integer"
        cur.execute(versql)
        ver = cur.fetchone()[0]
        if ver >= 120000:
            tabdefsql = """
WITH attrdef AS (
    SELECT
        n.nspname,
        c.relname,
        pg_catalog.array_to_string(c.reloptions || array(select 'toast.' || x from pg_catalog.unnest(tc.reloptions) x), ', ') as relopts,
        c.relpersistence,
        a.attnum,
        a.attname,
        pg_catalog.format_type(a.atttypid, a.atttypmod) as atttype,
        (SELECT substring(pg_catalog.pg_get_expr(d.adbin, d.adrelid, true) for 128) FROM pg_catalog.pg_attrdef d
            WHERE d.adrelid = a.attrelid AND d.adnum = a.attnum AND a.atthasdef) as attdefault,
        a.attnotnull,
        (SELECT c.collname FROM pg_catalog.pg_collation c, pg_catalog.pg_type t
            WHERE c.oid = a.attcollation AND t.oid = a.atttypid AND a.attcollation <> t.typcollation) as attcollation,
        a.attidentity,
        a.attgenerated
    FROM pg_catalog.pg_attribute a
    JOIN pg_catalog.pg_class c ON a.attrelid = c.oid
    JOIN pg_catalog.pg_namespace n ON c.relnamespace = n.oid
    LEFT JOIN pg_catalog.pg_class tc ON (c.reltoastrelid = tc.oid)
    WHERE a.attrelid = %(taboid)s
        AND a.attnum > 0
        AND NOT a.attisdropped
    ORDER BY a.attnum
),
coldef AS (
    SELECT
        attrdef.nspname,
        attrdef.relname,
        attrdef.relopts,
        attrdef.relpersistence,
        pg_catalog.format(
            '%%I %%s%%s%%s%%s%%s',
            attrdef.attname,
            attrdef.atttype,
            case when attrdef.attcollation is null then '' else pg_catalog.format(' COLLATE %%I', attrdef.attcollation) end,
            case when attrdef.attnotnull then ' NOT NULL' else '' end,
            case when attrdef.attdefault is null then ''
                else case when attrdef.attgenerated = 's' then pg_catalog.format(' GENERATED ALWAYS AS (%%s) STORED', attrdef.attdefault)
                    when attrdef.attgenerated <> '' then ' GENERATED AS NOT_IMPLEMENTED'
                    else pg_catalog.format(' DEFAULT %%s', attrdef.attdefault)
                end
            end,
            case when attrdef.attidentity<>'' then pg_catalog.format(' GENERATED %%s AS IDENTITY',
                    case attrdef.attidentity when 'd' then 'BY DEFAULT' when 'a' then 'ALWAYS' else 'NOT_IMPLEMENTED' end)
                else '' end
        ) as col_create_sql
    FROM attrdef
    ORDER BY attrdef.attnum
),
tabdef AS (
    SELECT
        coldef.nspname,
        coldef.relname,
        coldef.relopts,
        coldef.relpersistence,
        string_agg(coldef.col_create_sql, E',\n    ') as cols_create_sql
    FROM coldef
    GROUP BY
        coldef.nspname, coldef.relname, coldef.relopts, coldef.relpersistence
)
SELECT
    format(
        E'CREATE%%s TABLE %%I.%%I (\n    %%s\n)%%s;',
        case tabdef.relpersistence when 't' then ' TEMP' when 'u' then ' UNLOGGED' else '' end,
        tabdef.nspname,
        tabdef.relname,
        tabdef.cols_create_sql,
        case when tabdef.relopts <> '' then format(' WITH (%%s)', tabdef.relopts) else '' end
    ) as table_create_sql
FROM tabdef
            """
        else:
            tabdefsql = """
WITH attrdef AS (
    SELECT
        n.nspname,
        c.relname,
        pg_catalog.array_to_string(c.reloptions || array(select 'toast.' || x from pg_catalog.unnest(tc.reloptions) x), ', ') as relopts,
        c.relpersistence,
        a.attnum,
        a.attname,
        pg_catalog.format_type(a.atttypid, a.atttypmod) as atttype,
        (SELECT substring(pg_catalog.pg_get_expr(d.adbin, d.adrelid, true) for 128) FROM pg_catalog.pg_attrdef d
            WHERE d.adrelid = a.attrelid AND d.adnum = a.attnum AND a.atthasdef) as attdefault,
        a.attnotnull,
        (SELECT c.collname FROM pg_catalog.pg_collation c, pg_catalog.pg_type t
            WHERE c.oid = a.attcollation AND t.oid = a.atttypid AND a.attcollation <> t.typcollation) as attcollation,
        a.attidentity
    FROM pg_catalog.pg_attribute a
    JOIN pg_catalog.pg_class c ON a.attrelid = c.oid
    JOIN pg_catalog.pg_namespace n ON c.relnamespace = n.oid
    LEFT JOIN pg_catalog.pg_class tc ON (c.reltoastrelid = tc.oid)
    WHERE a.attrelid = %(taboid)s
        AND a.attnum > 0
        AND NOT a.attisdropped
    ORDER BY a.attnum
),
coldef AS (
    SELECT
        attrdef.nspname,
        attrdef.relname,
        attrdef.relopts,
        attrdef.relpersistence,
        pg_catalog.format(
            '%%I %%s%%s%%s%%s%%s',
            attrdef.attname,
            attrdef.atttype,
            case when attrdef.attcollation is null then '' else pg_catalog.format(' COLLATE %%I', attrdef.attcollation) end,
            case when attrdef.attnotnull then ' NOT NULL' else '' end,
            case when attrdef.attdefault is null then '' else pg_catalog.format(' DEFAULT %%s', attrdef.attdefault) end,
            case when attrdef.attidentity<>'' then pg_catalog.format(' GENERATED %%s AS IDENTITY',
                    case attrdef.attidentity when 'd' then 'BY DEFAULT' when 'a' then 'ALWAYS' else 'NOT_IMPLEMENTED' end)
                else '' end
        ) as col_create_sql
    FROM attrdef
    ORDER BY attrdef.attnum
),
tabdef AS (
    SELECT
        coldef.nspname,
        coldef.relname,
        coldef.relopts,
        coldef.relpersistence,
        string_agg(coldef.col_create_sql, E',\n    ') as cols_create_sql
    FROM coldef
    GROUP BY
        coldef.nspname, coldef.relname, coldef.relopts, coldef.relpersistence
)
SELECT
    format(
        E'CREATE%%s TABLE %%I.%%I (\n    %%s\n)%%s;',
        case tabdef.relpersistence when 't' then ' TEMP' when 'u' then ' UNLOGGED' else '' end,
        tabdef.nspname,
        tabdef.relname,
        tabdef.cols_create_sql,
        case when tabdef.relopts <> '' then format(' WITH (%%s)', tabdef.relopts) else '' end
    ) as table_create_sql
FROM tabdef
            """
        cur.execute(tabdefsql, {'taboid': taboid})
        defn = cur.fetchone()[0]
        return defn


if __name__ == "__main__":
    main()
